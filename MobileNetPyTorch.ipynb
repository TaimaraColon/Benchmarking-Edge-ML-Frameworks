{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Benchmark on CPU ---\n",
      "Warming up for 10 iterations on CPU...\n",
      "Warm-up complete. Starting benchmark...\n",
      "\n",
      "Results: Latency=1110.828 ms, Throughput=57.61 FPS\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Starting Benchmark on CUDA:0 ---\n",
      "Warming up for 10 iterations on CUDA...\n",
      "Warm-up complete. Starting benchmark...\n",
      "\n",
      "Results: Latency=38.719 ms, Throughput=1652.92 FPS\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "--- DETAILED PYTORCH NATIVE BENCHMARK RESULTS (BASELINE) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Hardware</th>\n",
       "      <th>Inf. Device</th>\n",
       "      <th>Framework/Precision</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Model Size (FP32) (MB)</th>\n",
       "      <th>Total Parameters</th>\n",
       "      <th>Model Load Time (ms)</th>\n",
       "      <th>Compile Time (ms)</th>\n",
       "      <th>Avg. Latency (P50) (ms)</th>\n",
       "      <th>Avg. Latency (Mean) (ms)</th>\n",
       "      <th>Worst-Case Latency (P99) (ms)</th>\n",
       "      <th>Throughput (FPS)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPU</td>\n",
       "      <td>CPU</td>\n",
       "      <td>PyTorch (Native) (FP32)</td>\n",
       "      <td>64</td>\n",
       "      <td>13.37</td>\n",
       "      <td>3,504,872</td>\n",
       "      <td>46.36</td>\n",
       "      <td>N/A (Native)</td>\n",
       "      <td>1110.619</td>\n",
       "      <td>1110.828</td>\n",
       "      <td>1176.451</td>\n",
       "      <td>57.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVIDIA GeForce RTX 3060 Laptop GPU</td>\n",
       "      <td>CUDA</td>\n",
       "      <td>PyTorch (Native) (FP32)</td>\n",
       "      <td>64</td>\n",
       "      <td>13.37</td>\n",
       "      <td>3,504,872</td>\n",
       "      <td>46.36</td>\n",
       "      <td>N/A (Native)</td>\n",
       "      <td>38.692</td>\n",
       "      <td>38.719</td>\n",
       "      <td>39.000</td>\n",
       "      <td>1652.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Target Hardware Inf. Device      Framework/Precision  \\\n",
       "0                                 CPU         CPU  PyTorch (Native) (FP32)   \n",
       "1  NVIDIA GeForce RTX 3060 Laptop GPU        CUDA  PyTorch (Native) (FP32)   \n",
       "\n",
       "   Batch Size Model Size (FP32) (MB) Total Parameters Model Load Time (ms)  \\\n",
       "0          64                  13.37        3,504,872                46.36   \n",
       "1          64                  13.37        3,504,872                46.36   \n",
       "\n",
       "  Compile Time (ms) Avg. Latency (P50) (ms) Avg. Latency (Mean) (ms)  \\\n",
       "0      N/A (Native)                1110.619                 1110.828   \n",
       "1      N/A (Native)                  38.692                   38.719   \n",
       "\n",
       "  Worst-Case Latency (P99) (ms) Throughput (FPS)  \n",
       "0                      1176.451            57.61  \n",
       "1                        39.000          1652.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Global Configuration ---\n",
    "BATCH_SIZE = 64 \n",
    "NUM_WARMUP = 10\n",
    "NUM_BENCHMARK = 100\n",
    "MODEL_NAME = \"MobileNetV2\"\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "DEVICE_NAME_CUDA = torch.cuda.get_device_name(0) if HAS_CUDA else \"N/A\"\n",
    "DEVICE_NAME_CPU = \"CPU\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# METRIC 1 & 2: MODEL LOAD TIME & MODEL SIZE CALCULATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def calculate_static_metrics():\n",
    "    \"\"\"Calculates model load time and size metrics. Loads model onto CPU initially.\"\"\"\n",
    "    \n",
    "    start_load_time = time.perf_counter()\n",
    "\n",
    "    torch_model = torchvision.models.mobilenet_v2(\n",
    "        weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    torch_model.eval()\n",
    "    \n",
    "    model_load_time_ms = (time.perf_counter() - start_load_time) * 1000\n",
    "\n",
    "    total_params = sum(p.numel() for p in torch_model.parameters())\n",
    "    \n",
    "    # *** DISPLAYING THIS NOW: FP32 size: 4 bytes per parameter ***\n",
    "    model_size_mb_fp32 = (total_params * 4) / (1024 * 1024)\n",
    "    \n",
    "    # We still calculate FP16 size just in case, but won't display it directly.\n",
    "    model_size_mb_fp16 = (total_params * 2) / (1024 * 1024)\n",
    "    \n",
    "    return torch_model, model_load_time_ms, total_params, model_size_mb_fp32, model_size_mb_fp16\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# CORE BENCHMARKING FUNCTION (Internal logic)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def _run_single_benchmark(model, device_name, num_warmup, num_benchmark):\n",
    "    \"\"\"\n",
    "    Executes the benchmark for a single device (CPU or CUDA).\n",
    "    Returns a dictionary of metrics for that device.\n",
    "    \"\"\"\n",
    "    device = torch.device(device_name)\n",
    "    timings = [] \n",
    "    \n",
    "    model.to(device)\n",
    "    input_tensor = torch.randn(BATCH_SIZE, 3, 224, 224).to(device)\n",
    "    \n",
    "    print(f\"\\n--- Starting Benchmark on {device_name.upper()} ---\")\n",
    "    \n",
    "    # --- CUDA TIMING LOGIC ---\n",
    "    if device.type == 'cuda':\n",
    "        starter = torch.cuda.Event(enable_timing=True)\n",
    "        ender = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        print(f\"Warming up for {num_warmup} iterations on CUDA...\")\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup):\n",
    "                _ = model(input_tensor)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Warm-up complete. Starting benchmark...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_benchmark):\n",
    "                starter.record()\n",
    "                _ = model(input_tensor)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize() \n",
    "                \n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time) \n",
    "\n",
    "    # --- CPU TIMING LOGIC ---\n",
    "    else: \n",
    "        print(f\"Warming up for {num_warmup} iterations on CPU...\")\n",
    "        \n",
    "        for _ in range(num_warmup):\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_tensor) \n",
    "            \n",
    "        print(\"Warm-up complete. Starting benchmark...\")\n",
    "\n",
    "        for _ in range(num_benchmark):\n",
    "            start_time = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_tensor)\n",
    "            end_time = time.perf_counter()\n",
    "            timings.append((end_time - start_time) * 1000)\n",
    "\n",
    "    # --- STATISTICAL CALCULATIONS ---\n",
    "    timings_np = np.array(timings)\n",
    "    \n",
    "    mean_time_ms = timings_np.mean()\n",
    "    median_latency = np.percentile(timings_np, 50)\n",
    "    p99_latency = np.percentile(timings_np, 99)\n",
    "    throughput_fps = (BATCH_SIZE / mean_time_ms) * 1000\n",
    "\n",
    "    print(f\"\\nResults: Latency={mean_time_ms:.3f} ms, Throughput={throughput_fps:.2f} FPS\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    return {\n",
    "        'Framework': 'PyTorch (Native)',\n",
    "        'Inf. Device': device.type.upper(),\n",
    "        'Hardware Details': DEVICE_NAME_CUDA if device.type == 'cuda' else DEVICE_NAME_CPU,\n",
    "        'Precision': 'FP32',\n",
    "        'Avg. Latency (P50) (ms)': median_latency,\n",
    "        'Avg. Latency (Mean) (ms)': mean_time_ms,\n",
    "        'Worst-Case Latency (P99) (ms)': p99_latency,\n",
    "        'Throughput (FPS)': throughput_fps,\n",
    "        'Batch Size': BATCH_SIZE\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# MASTER BENCHMARK & TABLE GENERATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def run_benchmarks_and_generate_table(model, static_metrics):\n",
    "    \"\"\"Runs all target benchmarks and generates the comparison table.\"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    # 1. Run CPU Benchmark\n",
    "    cpu_metrics = _run_single_benchmark(model, 'cpu', NUM_WARMUP, NUM_BENCHMARK)\n",
    "    all_results.append(cpu_metrics)\n",
    "\n",
    "    # 2. Run CUDA Benchmark (if available)\n",
    "    if HAS_CUDA:\n",
    "        cuda_metrics = _run_single_benchmark(model, 'cuda:0', NUM_WARMUP, NUM_BENCHMARK)\n",
    "        all_results.append(cuda_metrics)\n",
    "    else:\n",
    "        print(\"\\nCUDA not available. Skipping CUDA benchmark.\")\n",
    "\n",
    "\n",
    "    # --- CONSOLIDATE AND DISPLAY TABLE ---\n",
    "    \n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    \n",
    "    final_data = []\n",
    "    for index, row in df_results.iterrows():\n",
    "        # Inject static metrics into the dynamic metrics for the final table rows\n",
    "        final_data.append({\n",
    "            'Target Hardware': row['Hardware Details'],\n",
    "            'Inf. Device': row['Inf. Device'],\n",
    "            'Framework/Precision': f\"{row['Framework']} ({row['Precision']})\",\n",
    "            'Batch Size': row['Batch Size'],\n",
    "            # *** UPDATED HERE: Displaying FP32 size, the actual size of the running model ***\n",
    "            'Model Size (FP32) (MB)': f\"{static_metrics['model_size_mb_fp32']:.2f}\", \n",
    "            'Total Parameters': f\"{static_metrics['total_params']:,}\",\n",
    "            'Model Load Time (ms)': f\"{static_metrics['model_load_time_ms']:.2f}\",\n",
    "            'Compile Time (ms)': 'N/A (Native)',\n",
    "            'Avg. Latency (P50) (ms)': f\"{row['Avg. Latency (P50) (ms)']:.3f}\",\n",
    "            'Avg. Latency (Mean) (ms)': f\"{row['Avg. Latency (Mean) (ms)']:.3f}\",\n",
    "            'Worst-Case Latency (P99) (ms)': f\"{row['Worst-Case Latency (P99) (ms)']:.3f}\",\n",
    "            'Throughput (FPS)': f\"{row['Throughput (FPS)']:.2f}\",\n",
    "        })\n",
    "\n",
    "    df_final = pd.DataFrame(final_data)\n",
    "    \n",
    "    # Reorder columns for presentation\n",
    "    column_order = [\n",
    "        'Target Hardware',\n",
    "        'Inf. Device',\n",
    "        'Framework/Precision',\n",
    "        'Batch Size',\n",
    "        'Model Size (FP32) (MB)', # UPDATED COLUMN NAME\n",
    "        'Total Parameters',\n",
    "        'Model Load Time (ms)',\n",
    "        'Compile Time (ms)',\n",
    "        'Avg. Latency (P50) (ms)', \n",
    "        'Avg. Latency (Mean) (ms)',\n",
    "        'Worst-Case Latency (P99) (ms)',\n",
    "        'Throughput (FPS)',\n",
    "    ]\n",
    "    \n",
    "    df_final = df_final[column_order]\n",
    "\n",
    "    print(\"\\n\\n--- DETAILED PYTORCH NATIVE BENCHMARK RESULTS (BASELINE) ---\")\n",
    "    display(df_final)\n",
    "\n",
    "# MAIN EXECUTION\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1. Setup (Static Metrics & Model Loading)\n",
    "    torch_model, model_load_time_ms, total_params, model_size_mb_fp32, model_size_mb_fp16 = calculate_static_metrics()\n",
    "    \n",
    "    static_metrics = {\n",
    "        'model_load_time_ms': model_load_time_ms,\n",
    "        'total_params': total_params,\n",
    "        'model_size_mb_fp32': model_size_mb_fp32, # Value used in final table\n",
    "        'model_size_mb_fp16': model_size_mb_fp16, \n",
    "    }\n",
    "\n",
    "    # 2. Benchmarking (Runs CPU and CUDA/GPU)\n",
    "    run_benchmarks_and_generate_table(torch_model, static_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7edcbd51d003786adb1b4e1f80280b9023b7f40ffcc7c654fe654e28bb9b8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
