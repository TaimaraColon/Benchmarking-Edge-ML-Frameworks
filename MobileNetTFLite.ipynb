{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully converted (simulated) to TFLite format: mobilenet_v2_fp32.tflite\n",
      "\n",
      "--- Starting TFLite Benchmark Simulation on Mobile/Embedded CPU ---\n",
      "Warming up TFLite for 10 iterations...\n",
      "Warm-up complete. Starting benchmark...\n",
      "\n",
      "Results: Latency=15.516 ms, Throughput=4124.89 FPS\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "--- TFLITE (FP32) BENCHMARK SIMULATION RESULTS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Hardware</th>\n",
       "      <th>Inf. Device</th>\n",
       "      <th>Framework/Precision</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Model Size (Target INT8) (MB)</th>\n",
       "      <th>Total Parameters</th>\n",
       "      <th>PyTorch Model Load Time (ms)</th>\n",
       "      <th>Model Export/Compile Time (ms)</th>\n",
       "      <th>Avg. Latency (P50) (ms)</th>\n",
       "      <th>Avg. Latency (Mean) (ms)</th>\n",
       "      <th>Worst-Case Latency (P99) (ms)</th>\n",
       "      <th>Throughput (FPS)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mobile/Embedded CPU</td>\n",
       "      <td>Embedded CPU</td>\n",
       "      <td>TFLite (Simulated) (FP32)</td>\n",
       "      <td>64</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3,504,872</td>\n",
       "      <td>97.32</td>\n",
       "      <td>502.57</td>\n",
       "      <td>15.515</td>\n",
       "      <td>15.516</td>\n",
       "      <td>16.526</td>\n",
       "      <td>4124.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Target Hardware   Inf. Device        Framework/Precision  Batch Size  \\\n",
       "0  Mobile/Embedded CPU  Embedded CPU  TFLite (Simulated) (FP32)          64   \n",
       "\n",
       "  Model Size (Target INT8) (MB) Total Parameters PyTorch Model Load Time (ms)  \\\n",
       "0                          3.34        3,504,872                        97.32   \n",
       "\n",
       "  Model Export/Compile Time (ms) Avg. Latency (P50) (ms)  \\\n",
       "0                         502.57                  15.515   \n",
       "\n",
       "  Avg. Latency (Mean) (ms) Worst-Case Latency (P99) (ms) Throughput (FPS)  \n",
       "0                   15.516                        16.526          4124.89  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "import os\n",
    "# The actual TFLite dependencies (tensorflow, onnx, onnx2tf) are needed for real execution.\n",
    "# This script simulates the execution time and metrics.\n",
    "\n",
    "# --- Global Configuration ---\n",
    "BATCH_SIZE = 64\n",
    "NUM_WARMUP = 10\n",
    "NUM_BENCHMARK = 100\n",
    "MODEL_NAME = \"MobileNetV2\"\n",
    "TFLITE_MODEL_PATH = \"mobilenet_v2_fp32.tflite\" # Target FP32 TFLite model file\n",
    "# TFLite is primarily targeted at mobile/embedded CPUs.\n",
    "TFLITE_DEVICE_NAME = \"Mobile/Embedded CPU\"\n",
    "\n",
    "# --- Dummy Input ---\n",
    "DUMMY_INPUT_SHAPE = (BATCH_SIZE, 3, 224, 224)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# METRIC 1: MODEL LOAD TIME & MODEL SIZE CALCULATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def calculate_static_metrics():\n",
    "    \"\"\"Calculates model size and loads the PyTorch model.\"\"\"\n",
    "\n",
    "    start_load_time = time.perf_counter()\n",
    "    torch_model = torchvision.models.mobilenet_v2(\n",
    "        weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    torch_model.eval()\n",
    "    model_load_time_ms = (time.perf_counter() - start_load_time) * 1000\n",
    "\n",
    "    total_params = sum(p.numel() for p in torch_model.parameters())\n",
    "\n",
    "    # Base size is FP32 (4 bytes/param). We report the INT8 size (1 byte/param) as TFLite's target.\n",
    "    model_size_mb_fp32 = (total_params * 4) / (1024 * 1024)\n",
    "    model_size_mb_int8 = (total_params * 1) / (1024 * 1024) # 1 byte per parameter\n",
    "\n",
    "    return torch_model, model_load_time_ms, total_params, model_size_mb_fp32, model_size_mb_int8\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 2: EXPORT MODEL TO TFLITE (Simulated)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def export_to_tflite(torch_model, static_metrics):\n",
    "    \"\"\"\n",
    "    Simulates the export process from PyTorch (via ONNX) to TFLite.\n",
    "    Actual conversion involves 'torch.onnx.export' followed by 'onnx2tf'.\n",
    "    \"\"\"\n",
    "    start_export_time = time.perf_counter()\n",
    "\n",
    "    # Simulate a realistic export time based on model complexity\n",
    "    # This time captures the graph parsing and conversion overhead (e.g., ONNX + TFLite conversion).\n",
    "    simulated_export_time_ms = 450.0 + (static_metrics['total_params'] / 100000) * 1.5\n",
    "    export_time_ms = (time.perf_counter() - start_export_time) * 1000 + simulated_export_time_ms\n",
    "\n",
    "    print(f\"Model successfully converted (simulated) to TFLite format: {TFLITE_MODEL_PATH}\")\n",
    "\n",
    "    return export_time_ms\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# CORE BENCHMARKING FUNCTION (TFLite Interpreter Simulation)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def _run_tflite_benchmark(model_path, num_warmup, num_benchmark):\n",
    "    \"\"\"\n",
    "    Simulates the benchmark using the TFLite Interpreter.\n",
    "    \"\"\"\n",
    "    timings = []\n",
    "\n",
    "    # Create NumPy input data for the TFLite Interpreter (input is always a NumPy array)\n",
    "    input_data = np.random.randn(*DUMMY_INPUT_SHAPE).astype(np.float32)\n",
    "\n",
    "    print(f\"\\n--- Starting TFLite Benchmark Simulation on {TFLITE_DEVICE_NAME} ---\")\n",
    "\n",
    "    # --- Warm-up (Simulated) ---\n",
    "    print(f\"Warming up TFLite for {num_warmup} iterations...\")\n",
    "    simulated_warmup_latency = 15.0 # ms/batch\n",
    "    for _ in range(num_warmup):\n",
    "        time.sleep(simulated_warmup_latency / 1000) # Simulate time\n",
    "    print(\"Warm-up complete. Starting benchmark...\")\n",
    "\n",
    "    # --- Benchmark (Simulated) ---\n",
    "    # Simulate a realistic optimized CPU inference time.\n",
    "    base_simulated_latency = 12.0 # ms/batch on a strong mobile CPU for MobileNetV2\n",
    "\n",
    "    for _ in range(num_benchmark):\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Simulate the inference call\n",
    "        time.sleep(base_simulated_latency / 1000)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        timings.append((end_time - start_time) * 1000)\n",
    "\n",
    "    # --- STATISTICAL CALCULATIONS ---\n",
    "    timings_np = np.array(timings)\n",
    "\n",
    "    mean_time_ms = timings_np.mean()\n",
    "    median_latency = np.percentile(timings_np, 50)\n",
    "    p99_latency = np.percentile(timings_np, 99)\n",
    "    throughput_fps = (BATCH_SIZE / mean_time_ms) * 1000\n",
    "\n",
    "    print(f\"\\nResults: Latency={mean_time_ms:.3f} ms, Throughput={throughput_fps:.2f} FPS\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    return {\n",
    "        'Framework': 'TFLite (Simulated)',\n",
    "        'Inf. Device': 'Embedded CPU',\n",
    "        'Precision': 'FP32',\n",
    "        'Avg. Latency (P50) (ms)': median_latency,\n",
    "        'Avg. Latency (Mean) (ms)': mean_time_ms,\n",
    "        'Worst-Case Latency (P99) (ms)': p99_latency,\n",
    "        'Throughput (FPS)': throughput_fps,\n",
    "        'Batch Size': BATCH_SIZE\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# MASTER BENCHMARK & TABLE GENERATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def run_benchmarks_and_generate_table(torch_model, static_metrics):\n",
    "    \"\"\"Runs the TFLite simulation and generates the table.\"\"\"\n",
    "\n",
    "    # 1. Export Model\n",
    "    export_time_ms = export_to_tflite(torch_model, static_metrics)\n",
    "\n",
    "    # 2. Run TFLite Benchmark Simulation\n",
    "    tflite_metrics = _run_tflite_benchmark(TFLITE_MODEL_PATH, NUM_WARMUP, NUM_BENCHMARK)\n",
    "\n",
    "    # --- CONSOLIDATE AND DISPLAY TABLE ---\n",
    "    final_data = []\n",
    "\n",
    "    # Inject static and export metrics into the dynamic metrics\n",
    "    final_data.append({\n",
    "        'Target Hardware': TFLITE_DEVICE_NAME,\n",
    "        'Inf. Device': tflite_metrics['Inf. Device'],\n",
    "        'Framework/Precision': f\"{tflite_metrics['Framework']} ({tflite_metrics['Precision']})\",\n",
    "        'Batch Size': tflite_metrics['Batch Size'],\n",
    "        # Reporting the target INT8 size, as this is TFLite's main optimization target\n",
    "        'Model Size (Target INT8) (MB)': f\"{static_metrics['model_size_mb_int8']:.2f}\",\n",
    "        'Total Parameters': f\"{static_metrics['total_params']:,}\",\n",
    "        'PyTorch Model Load Time (ms)': f\"{static_metrics['model_load_time_ms']:.2f}\",\n",
    "        'Model Export/Compile Time (ms)': f\"{export_time_ms:.2f}\",\n",
    "        'Avg. Latency (P50) (ms)': f\"{tflite_metrics['Avg. Latency (P50) (ms)']:.3f}\",\n",
    "        'Avg. Latency (Mean) (ms)': f\"{tflite_metrics['Avg. Latency (Mean) (ms)']:.3f}\",\n",
    "        'Worst-Case Latency (P99) (ms)': f\"{tflite_metrics['Worst-Case Latency (P99) (ms)']:.3f}\",\n",
    "        'Throughput (FPS)': f\"{tflite_metrics['Throughput (FPS)']:.2f}\",\n",
    "    })\n",
    "\n",
    "    df_final = pd.DataFrame(final_data)\n",
    "\n",
    "    # Reorder columns for presentation\n",
    "    column_order = [\n",
    "        'Target Hardware', 'Inf. Device', 'Framework/Precision', 'Batch Size',\n",
    "        'Model Size (Target INT8) (MB)',\n",
    "        'Total Parameters',\n",
    "        'PyTorch Model Load Time (ms)',\n",
    "        'Model Export/Compile Time (ms)',\n",
    "        'Avg. Latency (P50) (ms)',\n",
    "        'Avg. Latency (Mean) (ms)',\n",
    "        'Worst-Case Latency (P99) (ms)',\n",
    "        'Throughput (FPS)',\n",
    "    ]\n",
    "\n",
    "    df_final = df_final[column_order]\n",
    "\n",
    "    print(\"\\n\\n--- TFLITE (FP32) BENCHMARK SIMULATION RESULTS ---\")\n",
    "    display(df_final)\n",
    "\n",
    "# MAIN EXECUTION\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # 1. Setup (Static Metrics & Model Loading)\n",
    "    torch_model, model_load_time_ms, total_params, model_size_mb_fp32, model_size_mb_int8 = calculate_static_metrics()\n",
    "\n",
    "    static_metrics = {\n",
    "        'model_load_time_ms': model_load_time_ms,\n",
    "        'total_params': total_params,\n",
    "        'model_size_mb_fp32': model_size_mb_fp32,\n",
    "        'model_size_mb_int8': model_size_mb_int8, # Value used in final table\n",
    "    }\n",
    "\n",
    "    # 2. Benchmarking (Runs TFLite simulation)\n",
    "    run_benchmarks_and_generate_table(torch_model, static_metrics)\n",
    "\n",
    "    # Clean up simulation files\n",
    "    if os.path.exists(TFLITE_MODEL_PATH):\n",
    "        os.remove(TFLITE_MODEL_PATH)\n",
    "    if os.path.exists(\"mobilenet_v2.onnx\"):\n",
    "        os.remove(\"mobilenet_v2.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7edcbd51d003786adb1b4e1f80280b9023b7f40ffcc7c654fe654e28bb9b8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
