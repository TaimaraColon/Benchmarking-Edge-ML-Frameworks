{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why TorchVision MobileNetV2 is the Best Single Model for Cross-Framework Edge Benchmarking\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV2 (TorchVision, ImageNet-1k) is the most practical model choice to benchmark TensorRT, OpenVINO, TensorFlow Lite, and ExecuTorch. \n",
    "It’s (1) architected for mobile/edge efficiency, (2) pretrained, and (3) easy to export and run across all four runtimes. \n",
    "These traits let you isolate runtime differences (engines, kernels, quantization) minimizing the encounter of model-conversion issues.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Criteria & How MobileNetV2 Satisfies Them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Edge-oriented architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV2 introduces inverted residuals + linear bottlenecks and relies on convolutions which permits reduction of computation for on-device inference while maintaining accuracy. \n",
    "In the standard 224×224 setting it operates at ~300M multiply-adds with a small parameter count (aprox. 3–4M), making it ideal for Jetsons/CPUs. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Pretrained"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchVision provides official pretrained weights and built-in inference transforms (resize→center-crop→normalize), and offers a preprocessing process you can \n",
    "mirror in every runtime for fair comparisons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Accessible Export"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ONNX to deploy the PyTorch model to TensorRT/OpenVINO, torch.export for ExecuTorch, and a functionally equivalent Keras MobileNetV2 for TFLite; \n",
    "Note to report any small accuracy differences due to weight sources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Quantization support"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT (INT8), OpenVINO (PTQ), and TFLite (post-training quantization) all support straightforward INT8 workflows; \n",
    "MobileNet ops quantize well, enabling a precise study of accuracy-vs-latency/size trade-offs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do? : Load TorchVision MobileNetV2. Export to ONNX for TensorRT/OpenVINO and to ExecuTorch via torch.export. TFLite from the Keras MobileNetV2 path.\n",
    "\n",
    "Dataset: Use ImageNet-1k (ILSVRC 2012) validation set (50k images); can also evaluate a fixed, stratified subset (e.g., 5k). \n",
    "\n",
    "Precisions: Running MobileNetV2 at FP16/FP32/INT8 on TensorRT/TFLite/OpenVINO/ExecuTorch.\n",
    "\n",
    "Metrics: Report median/percentile latency, throughput, model size, inference time -load time, and accuracy. \n",
    "OpenVINO’s benchmark_app and TensorRT’s trtexec provide ready-made timing outputs. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to look out for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Preprocessing: Replicate TorchVision’s MobileNetV2 transforms (resize, center-crop, normalize) in every runtime to keep accuracy comparable. \n",
    "\n",
    "B) Quantization variance: Use the same calibration/representative set across TensorRT/OpenVINO/TFLite to make INT8 comparisons fair.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV2 (TorchVision) is the best single model for cross-framework edge benchmarking because it exports cleanly to TensorRT, OpenVINO, TFLite, and ExecuTorch. \n",
    "Using the same architecture and ImageNet-1k preprocessing isolates runtime differences.  \n",
    "Testing FP32/FP16/INT8 exposes clear trade-offs in latency, throughput, accuracy, and model size, giving a fair, repeatable comparison."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References/Documentation sites\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchVision MobileNetV2 docs (weights & transforms). \n",
    "docs.pytorch.org\n",
    "\n",
    "MobileNetV2 paper: inverted residuals, depthwise separable design, efficiency trade-offs. \n",
    "Open Access CVF\n",
    "\n",
    "PyTorch ONNX export overview. \n",
    "docs.pytorch.org\n",
    "\n",
    "TensorRT ONNX import & trtexec usage. \n",
    "NVIDIA Docs\n",
    "\n",
    "OpenVINO: convert ONNX→IR; benchmark_app (runs IR/ONNX). \n",
    "docs.openvino.ai\n",
    "\n",
    "TensorFlow Lite post-training quantization guide. \n",
    "TensorFlow\n",
    "\n",
    "ExecuTorch export and XNNPACK backend (CPU). \n",
    "docs.pytorch.org\n",
    "\n",
    "ImageNet-1k dataset (TorchVision & ILSVRC 2012). \n",
    "docs.pytorch.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5914b77f64058c283447d5a8074ae968f7e36af4428f93489237561bdf59b357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
