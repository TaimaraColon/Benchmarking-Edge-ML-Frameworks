{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled and dummy input moved to cuda:0 successfully.\n",
      "Warming up FP16 optimized model for 10 iterations on CUDA...\n",
      "Warm-up complete. Starting benchmark...\n",
      "\n",
      "--- Benchmark Results (CUDA @ BATCH=64, Precision: FP16/TensorRT) ---\n",
      "Inference Time (Avg over 100 runs): 23.341 ms\n",
      "Throughput (FPS): 2741.90 FPS\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- TABLE 1: DETAILED TENSORRT-STYLE (FP16) METRICS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "      <th>Unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Target Hardware</td>\n",
       "      <td>NVIDIA GeForce RTX 3060 Laptop GPU (TensorRT/F...</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inference Device</td>\n",
       "      <td>CUDA</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batch Size</td>\n",
       "      <td>64</td>\n",
       "      <td>Samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Size (FP16)</td>\n",
       "      <td>6.69</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Parameters</td>\n",
       "      <td>3,504,872</td>\n",
       "      <td>Params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Load Time</td>\n",
       "      <td>129.34</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TensorRT Compile Time</td>\n",
       "      <td>2629.61</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Avg. Latency (P50)</td>\n",
       "      <td>23.097</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Avg. Latency (Mean)</td>\n",
       "      <td>23.341</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Worst-Case Latency (P99)</td>\n",
       "      <td>24.762</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Throughput (FPS)</td>\n",
       "      <td>2741.90</td>\n",
       "      <td>FPS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Metric  \\\n",
       "0            Target Hardware   \n",
       "1           Inference Device   \n",
       "2                 Batch Size   \n",
       "3          Model Size (FP16)   \n",
       "4           Total Parameters   \n",
       "5            Model Load Time   \n",
       "6      TensorRT Compile Time   \n",
       "7         Avg. Latency (P50)   \n",
       "8        Avg. Latency (Mean)   \n",
       "9   Worst-Case Latency (P99)   \n",
       "10          Throughput (FPS)   \n",
       "\n",
       "                                                Value      Unit  \n",
       "0   NVIDIA GeForce RTX 3060 Laptop GPU (TensorRT/F...       N/A  \n",
       "1                                                CUDA       N/A  \n",
       "2                                                  64   Samples  \n",
       "3                                                6.69        MB  \n",
       "4                                           3,504,872    Params  \n",
       "5                                              129.34        ms  \n",
       "6                                             2629.61        ms  \n",
       "7                                              23.097  ms/batch  \n",
       "8                                              23.341  ms/batch  \n",
       "9                                              24.762  ms/batch  \n",
       "10                                            2741.90       FPS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- TABLE 2: PROJECT PLAN AND NEXT STEPS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Framework</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Measured Latency (ms)</th>\n",
       "      <th>Measured Throughput (FPS)</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PyTorch (Native, FP32)</td>\n",
       "      <td>FP32</td>\n",
       "      <td>N/A (Baseline)</td>\n",
       "      <td>N/A (Baseline)</td>\n",
       "      <td>Completed (Previous Run)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PyTorch (TensorRT/FP16 Sim.)</td>\n",
       "      <td>FP16</td>\n",
       "      <td>23.34</td>\n",
       "      <td>2741.90</td>\n",
       "      <td>Complete (This Run)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TensorRT (Full INT8)</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFLite (INT8)</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OpenVINO (FP32/FP16)</td>\n",
       "      <td>FP32/FP16</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ExecuTorch (INT8)</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Framework  Precision Measured Latency (ms)  \\\n",
       "0        PyTorch (Native, FP32)       FP32        N/A (Baseline)   \n",
       "1  PyTorch (TensorRT/FP16 Sim.)       FP16                 23.34   \n",
       "2          TensorRT (Full INT8)       INT8         N/A (Planned)   \n",
       "3                 TFLite (INT8)       INT8         N/A (Planned)   \n",
       "4          OpenVINO (FP32/FP16)  FP32/FP16         N/A (Planned)   \n",
       "5             ExecuTorch (INT8)       INT8         N/A (Planned)   \n",
       "\n",
       "  Measured Throughput (FPS)                    Status  \n",
       "0            N/A (Baseline)  Completed (Previous Run)  \n",
       "1                   2741.90       Complete (This Run)  \n",
       "2             N/A (Planned)               In Progress  \n",
       "3             N/A (Planned)                   Planned  \n",
       "4             N/A (Planned)                   Planned  \n",
       "5             N/A (Planned)                   Planned  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.jit # Needed for tracing, a key step in TensorRT preparation\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Global Configuration ---\n",
    "BATCH_SIZE = 64 \n",
    "NUM_WARMUP = 10\n",
    "NUM_BENCHMARK = 100\n",
    "# Target only CUDA, as TensorRT is GPU-specific\n",
    "TARGET_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"MobileNetV2 (FP16 Optimized)\"\n",
    "if TARGET_DEVICE.type == 'cuda':\n",
    "    # This fetches the specific GPU name (e.g., \"NVIDIA Jetson Nano\", \"RTX 3090\")\n",
    "    DEVICE_NAME = f\"{torch.cuda.get_device_name(0)} (TensorRT/FP16 Sim.)\"\n",
    "else:\n",
    "    # If no CUDA device, TensorRT optimization is not possible\n",
    "    DEVICE_NAME = \"CPU (No TensorRT Possible)\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# TENSORRT-STYLE COMPILATION (FP16 Conversion and JIT Trace)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def compile_to_tensorrt_fp16(model, dummy_input):\n",
    "    \"\"\"\n",
    "    Converts the PyTorch model to half-precision (FP16) and traces it \n",
    "    to create a TorchScript module, simulating the preparation for TensorRT.\n",
    "    \"\"\"\n",
    "    if TARGET_DEVICE.type != 'cuda':\n",
    "        print(\"Warning: Cannot perform FP16 optimization or TensorRT simulation on CPU.\")\n",
    "        return model, 0.0\n",
    "\n",
    "    # 1. Convert to Half Precision (FP16)\n",
    "    # This is the primary optimization step in TensorRT-style compilation\n",
    "    model_fp16 = model.half()\n",
    "    \n",
    "    # Move the dummy input to FP16 as well\n",
    "    dummy_input_fp16 = dummy_input.half()\n",
    "\n",
    "    # 2. Trace the model (simulates building an optimized graph)\n",
    "    # TensorRT works on a static computation graph. TorchScript tracing provides this.\n",
    "    start_compile_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        # Trace the FP16 model with FP16 input\n",
    "        traced_model = torch.jit.trace(model_fp16, dummy_input_fp16)\n",
    "    \n",
    "    compile_time_ms = (time.perf_counter() - start_compile_time) * 1000\n",
    "    \n",
    "    return traced_model, compile_time_ms\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# METRIC 1 & 2: MODEL LOAD TIME & MODEL SIZE CALCULATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def calculate_static_metrics(dummy_input):\n",
    "    \"\"\"Calculates model load time, size, and TensorRT compilation time.\"\"\"\n",
    "    \n",
    "    # Start timer for model loading\n",
    "    start_load_load_time = time.perf_counter()\n",
    "\n",
    "    # Load MobileNetV2 model (FP32 initially)\n",
    "    torch_model = torchvision.models.mobilenet_v2(\n",
    "        weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    torch_model.eval()\n",
    "\n",
    "    # Move model to the TARGET_DEVICE (e.g., CUDA)\n",
    "    torch_model.to(TARGET_DEVICE)\n",
    "\n",
    "    # Calculate load time\n",
    "    model_load_time_ms = (time.perf_counter() - start_load_load_time) * 1000\n",
    "\n",
    "    # Calculate model size (Static Metric)\n",
    "    total_params = sum(p.numel() for p in torch_model.parameters())\n",
    "    # Assuming FP16 optimization: 2 bytes per parameter. Convert to Megabytes.\n",
    "    model_size_mb = (total_params * 2) / (1024 * 1024)\n",
    "    \n",
    "    # Compile the model to the TensorRT/FP16 equivalent\n",
    "    tensorrt_model, compile_time_ms = compile_to_tensorrt_fp16(torch_model, dummy_input)\n",
    "\n",
    "    return tensorrt_model, model_load_time_ms, model_size_mb, total_params, compile_time_ms\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# BENCHMARKING FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def benchmark_model(model, input_tensor, num_warmup, num_benchmark):\n",
    "    \"\"\"\n",
    "    Benchmarks the TensorRT-optimized model inference time.\n",
    "    The model is assumed to be an FP16 JIT-traced CUDA model.\n",
    "    \"\"\"\n",
    "    timings = [] # List to store all individual run times\n",
    "    device_type = input_tensor.device.type\n",
    "    \n",
    "    if device_type != 'cuda':\n",
    "        # If running on CPU, use the basic CPU timing logic\n",
    "        print(\"Running non-optimized benchmark on CPU...\")\n",
    "        \n",
    "        # Warm-up (CPU)\n",
    "        for _ in range(num_warmup):\n",
    "             with torch.no_grad():\n",
    "                _ = model(input_tensor) \n",
    "        \n",
    "        # Measure performance (CPU)\n",
    "        for _ in range(num_benchmark):\n",
    "            start_time = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_tensor)\n",
    "            end_time = time.perf_counter()\n",
    "            timings.append((end_time - start_time) * 1000) # Time in milliseconds (ms)\n",
    "\n",
    "    else: # CUDA Timing Logic (for FP16/TensorRT)\n",
    "        \n",
    "        # The input tensor must be FP16 to match the compiled model\n",
    "        input_tensor_fp16 = input_tensor.half() \n",
    "        \n",
    "        starter = torch.cuda.Event(enable_timing=True)\n",
    "        ender = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        print(f\"Warming up FP16 optimized model for {num_warmup} iterations on CUDA...\")\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup):\n",
    "                _ = model(input_tensor_fp16)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Warm-up complete. Starting benchmark...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_benchmark):\n",
    "                starter.record()\n",
    "                _ = model(input_tensor_fp16)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize() # Wait for GPU\n",
    "                \n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time) # Time is in milliseconds (ms)\n",
    "\n",
    "    # --- STATISTICAL CALCULATIONS ---\n",
    "    timings_np = np.array(timings)\n",
    "    \n",
    "    mean_time_ms = timings_np.mean()\n",
    "    std_time_ms = timings_np.std()\n",
    "    \n",
    "    median_latency = np.percentile(timings_np, 50)\n",
    "    p90_latency = np.percentile(timings_np, 90)\n",
    "    p99_latency = np.percentile(timings_np, 99)\n",
    "    \n",
    "    # METRIC 4: THROUGHPUT (FPS)\n",
    "    throughput_fps = (BATCH_SIZE / mean_time_ms) * 1000\n",
    "\n",
    "    print(f\"\\n--- Benchmark Results ({device_type.upper()} @ BATCH={input_tensor.shape[0]}, Precision: FP16/TensorRT) ---\")\n",
    "    print(f\"Inference Time (Avg over {num_benchmark} runs): {mean_time_ms:.3f} ms\")\n",
    "    print(f\"Throughput (FPS): {throughput_fps:.2f} FPS\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    return {\n",
    "        'mean_time_ms': mean_time_ms,\n",
    "        'std_time_ms': std_time_ms,\n",
    "        'median_latency': median_latency,\n",
    "        'p90_latency': p90_latency,\n",
    "        'p99_latency': p99_latency,\n",
    "        'throughput_fps': throughput_fps,\n",
    "        'device_type': device_type,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# PANDAS TABLE GENERATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def generate_presentation_tables(static_metrics, dynamic_metrics):\n",
    "    \"\"\"Generates and prints the two presentation tables.\"\"\"\n",
    "    \n",
    "    # --- TABLE 1: DETAILED BASELINE METRICS (FINDINGS) ---\n",
    "    \n",
    "    metrics_data = {\n",
    "        'Metric': [\n",
    "            'Target Hardware', 'Inference Device', 'Batch Size', 'Model Size (FP16)', \n",
    "            'Total Parameters', 'Model Load Time', 'TensorRT Compile Time', \n",
    "            'Avg. Latency (P50)', 'Avg. Latency (Mean)', 'Worst-Case Latency (P99)', \n",
    "            'Throughput (FPS)'\n",
    "        ],\n",
    "        'Value': [\n",
    "            DEVICE_NAME, \n",
    "            dynamic_metrics['device_type'].upper(), \n",
    "            dynamic_metrics['batch_size'], \n",
    "            f\"{static_metrics['model_size_mb']:.2f}\",\n",
    "            f\"{static_metrics['total_params']:,}\",\n",
    "            f\"{static_metrics['model_load_time_ms']:.2f}\",\n",
    "            f\"{static_metrics['compile_time_ms']:.2f}\",\n",
    "            f\"{dynamic_metrics['median_latency']:.3f}\",\n",
    "            f\"{dynamic_metrics['mean_time_ms']:.3f}\",\n",
    "            f\"{dynamic_metrics['p99_latency']:.3f}\",\n",
    "            f\"{dynamic_metrics['throughput_fps']:.2f}\"\n",
    "        ],\n",
    "        'Unit': [\n",
    "            'N/A', 'N/A', 'Samples', 'MB', 'Params', 'ms', 'ms', 'ms/batch', 'ms/batch', 'ms/batch', 'FPS'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_data)\n",
    "    print(\"\\n\\n--- TABLE 1: DETAILED TENSORRT-STYLE (FP16) METRICS ---\")\n",
    "    display(df_metrics)\n",
    "    \n",
    "    # --- TABLE 2: PROJECT PLAN (NEXT STEPS) ---\n",
    "\n",
    "\n",
    "    next_steps_data = {\n",
    "        'Framework': [\n",
    "            'PyTorch (Native, FP32)', \n",
    "            'PyTorch (TensorRT/FP16 Sim.)', # This is the current benchmark\n",
    "            'TensorRT (Full INT8)', \n",
    "            'TFLite (INT8)', \n",
    "            'OpenVINO (FP32/FP16)', \n",
    "            'ExecuTorch (INT8)'\n",
    "        ],\n",
    "        'Precision': [\n",
    "            f'FP32', \n",
    "            'FP16', \n",
    "            'INT8', \n",
    "            'INT8', \n",
    "            'FP32/FP16', \n",
    "            'INT8'\n",
    "        ],\n",
    "        'Measured Latency (ms)': [\n",
    "            'N/A (Baseline)', # Placeholder for comparison with previous run\n",
    "            f'{dynamic_metrics[\"mean_time_ms\"]:.2f}', # Measured Value\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)'\n",
    "        ],\n",
    "        'Measured Throughput (FPS)': [\n",
    "            'N/A (Baseline)', # Placeholder for comparison with previous run\n",
    "            f'{dynamic_metrics[\"throughput_fps\"]:.2f}', # Measured Value\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)'\n",
    "        ],\n",
    "        'Status': [\n",
    "            'Completed (Previous Run)',\n",
    "            'Complete (This Run)',\n",
    "            'In Progress',\n",
    "            'Planned',\n",
    "            'Planned',\n",
    "            'Planned'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_plan = pd.DataFrame(next_steps_data)\n",
    "    print(\"\\n\\n--- TABLE 2: PROJECT PLAN AND NEXT STEPS ---\")\n",
    "    display(df_plan)\n",
    "\n",
    "# MAIN\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1. Setup Dummy Input before compilation\n",
    "    # Dummy input must be FP32 initially before being passed to compilation\n",
    "    dummy_input_fp32 = torch.randn(BATCH_SIZE, 3, 224, 224).to(TARGET_DEVICE)\n",
    "    \n",
    "    # 2. Setup and Compile\n",
    "    tensorrt_model, model_load_time_ms, model_size_mb, total_params, compile_time_ms = calculate_static_metrics(dummy_input_fp32)\n",
    "    \n",
    "    static_metrics = {\n",
    "        'model_load_time_ms': model_load_time_ms,\n",
    "        'model_size_mb': model_size_mb,\n",
    "        'total_params': total_params,\n",
    "        'compile_time_ms': compile_time_ms,\n",
    "    }\n",
    "\n",
    "    print(f\"Model compiled and dummy input moved to {TARGET_DEVICE} successfully.\")\n",
    "\n",
    "    # 3. Benchmarking\n",
    "    # Pass the FP32 dummy input to the benchmark function, which handles FP16 conversion internally\n",
    "    dynamic_metrics = benchmark_model(tensorrt_model, dummy_input_fp32, NUM_WARMUP, NUM_BENCHMARK)\n",
    "    \n",
    "    # 4. Generate Tables\n",
    "    generate_presentation_tables(static_metrics, dynamic_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7edcbd51d003786adb1b4e1f80280b9023b7f40ffcc7c654fe654e28bb9b8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
