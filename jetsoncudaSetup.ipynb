{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step-by-Step Guide: Installing PyTorch with CUDA on NVIDIA Jetson**\n",
    "\n",
    "Installation on NVIDIA Jetson devices (Nano, Xavier, Orin) is different from standard x86 desktops because the Jetson uses an ARM architecture and relies on the specialized JetPack SDK. You cannot use the standard `pip install` commands from PyTorch.org.\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "**1. NVIDIA Jetson Device:** Running the latest version of the JetPack SDK.\n",
    "\n",
    "**2. Internet Connection:** Required for downloading packages.\n",
    "\n",
    "**3. Terminal Access:** Via SSH or directly on the device.\n",
    "\n",
    "## **Step 1: Determine Your JetPack Version**\n",
    "\n",
    "The most critical step is identifying your running JetPack version, as the PyTorch wheel file (`.whl`) must be built specifically for it.\n",
    "\n",
    "**1. Open a terminal on your Jetson.**\n",
    "\n",
    "**2. Run the following command to check your operating system information, which usually includes the JetPack version (e.g., 4.6.1, 5.1.2):**\n",
    "\n",
    "`cat /etc/nv_tegra_release`\n",
    "\n",
    "\n",
    "*Note the major version (e.g., 5.x or 6.x).*\n",
    "\n",
    "## **Step 2: Locate the Official NVIDIA PyTorch Wheel (.whl)**\n",
    "\n",
    "NVIDIA provides custom PyTorch builds for the Jetson architecture. You must find the wheel that matches your JetPack version.\n",
    "\n",
    "Navigate to the official **NVIDIA Developer Forum for PyTorch on Jetson**. (A quick search for \"PyTorch Jetson\" will usually lead you to the correct thread.)\n",
    "\n",
    "In that thread, look for the most recent post that provides download links. It will specify which PyTorch version works with which JetPack version.\n",
    "\n",
    "Find the `wget` link for the PyTorch and, optionally, the TorchVision .whl file that corresponds to your system.\n",
    "\n",
    "## **Step 3: Install Required Dependencies**\n",
    "\n",
    "Before installing PyTorch, install required system dependencies:\n",
    "\n",
    "\n",
    "`sudo apt update`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sudo apt install python3-pip libopenblas-base libjpeg-dev zlib1g-dev`\n",
    "\n",
    "\n",
    "## **Step 4: Download and Install the PyTorch Wheel**\n",
    "\n",
    "Use the `wget` link you found in Step 2 to download the wheel file to your Jetson, and then install it using `pip3`.\n",
    "\n",
    "**1. Download the PyTorch Wheel:**\n",
    "\n",
    "`# Replace [URL_TO_PYTORCH.whl] with the link you found`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wget [URL_TO_PYTORCH.whl]`\n",
    "\n",
    "\n",
    "**2. Install the PyTorch Wheel:**\n",
    "\n",
    "`# Replace [FILENAME.whl] with the downloaded file's name`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip3 install [FILENAME.whl]`\n",
    "\n",
    "\n",
    "## **Step 5: Install TorchVision**\n",
    "\n",
    "If you need TorchVision (we do need it for MobileNetV2), you often need to build it from source or use a pre-compiled wheel due to architecture differences.\n",
    "\n",
    "**1. Install dependencies for TorchVision:**\n",
    "\n",
    "`sudo apt install libxml2-dev libxslt1-dev libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev`\n",
    "\n",
    "\n",
    "**2. Download and Install TorchVision: (Again, check the NVIDIA forum for the correct TorchVision .whl file link for your JetPack version and repeat Step 4 using those links.)**\n",
    "\n",
    "## **Step 6: Verification**\n",
    "Finally, verify the installation by running Python and checking CUDA availability:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch and CUDA Setup Diagnosis ---\n",
      "PyTorch Version: 2.0.1+cu117\n",
      "Python Version: 3.9.13\n",
      "----------------------------------------\n",
      "PyTorch built with CUDA version: 11.7\n",
      "torch.cuda.is_available(): True\n",
      "----------------------------------------\n",
      "Number of GPUs detected: 1\n",
      "GPU 0 Name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "GPU 0 Capability: (8, 6)\n",
      "Total Memory: 6.00 GB\n",
      "----------------------------------------\n",
      "torch.backends.cudnn.is_available(): True\n",
      "Test tensor successfully moved to device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "def check_cuda_setup():\n",
    "    \"\"\"Checks the PyTorch and CUDA installation status.\"\"\"\n",
    "    \n",
    "    print(\"--- PyTorch and CUDA Setup Diagnosis ---\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check 1: Is PyTorch built with CUDA support?\n",
    "    # This checks which CUDA version PyTorch was compiled against.\n",
    "    cuda_build_version = torch.version.cuda\n",
    "    print(f\"PyTorch built with CUDA version: {cuda_build_version}\")\n",
    "    \n",
    "    # Check 2: Is CUDA currently available for use?\n",
    "    cuda_is_available = torch.cuda.is_available()\n",
    "    print(f\"torch.cuda.is_available(): {cuda_is_available}\")\n",
    "\n",
    "    if not cuda_is_available:\n",
    "        print(\"\\n\\t*** DIAGNOSIS: CUDA is NOT available to PyTorch. ***\")\n",
    "        print(\"\\tCheck your PyTorch installation (did you use the CUDA version?)\")\n",
    "        print(\"\\tCheck your NVIDIA drivers and CUDA Toolkit installation.\")\n",
    "        return\n",
    "\n",
    "    # If CUDA is available, check GPU details\n",
    "    print(\"-\" * 40)\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs detected: {gpu_count}\")\n",
    "    \n",
    "    if gpu_count > 0:\n",
    "        for i in range(gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            print(f\"GPU {i} Name: {gpu_name}\")\n",
    "            print(f\"GPU {i} Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "            print(f\"Total Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Check 3: CuDNN status (often used for performance)\n",
    "    cudnn_is_available = torch.backends.cudnn.is_available()\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"torch.backends.cudnn.is_available(): {cudnn_is_available}\")\n",
    "\n",
    "    if cuda_is_available and gpu_count > 0:\n",
    "        # Final check: can we move a tensor to the device?\n",
    "        try:\n",
    "            test_tensor = torch.randn(2, 2).to('cuda:0')\n",
    "            print(f\"Test tensor successfully moved to device: {test_tensor.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move test tensor to GPU: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_cuda_setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this check passes, your Jetson is properly configured for GPU-accelerated PyTorch workloads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7edcbd51d003786adb1b4e1f80280b9023b7f40ffcc7c654fe654e28bb9b8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
