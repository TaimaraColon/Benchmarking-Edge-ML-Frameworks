{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch and CUDA Setup Diagnosis ---\n",
      "PyTorch Version: 2.0.1+cu117\n",
      "Python Version: 3.9.13\n",
      "----------------------------------------\n",
      "PyTorch built with CUDA version: 11.7\n",
      "torch.cuda.is_available(): True\n",
      "----------------------------------------\n",
      "Number of GPUs detected: 1\n",
      "GPU 0 Name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "GPU 0 Capability: (8, 6)\n",
      "Total Memory: 6.00 GB\n",
      "----------------------------------------\n",
      "torch.backends.cudnn.is_available(): True\n",
      "Test tensor successfully moved to device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "def check_cuda_setup():\n",
    "    \"\"\"Checks the PyTorch and CUDA installation status.\"\"\"\n",
    "    \n",
    "    print(\"--- PyTorch and CUDA Setup Diagnosis ---\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check 1: Is PyTorch built with CUDA support?\n",
    "    # This checks which CUDA version PyTorch was compiled against.\n",
    "    cuda_build_version = torch.version.cuda\n",
    "    print(f\"PyTorch built with CUDA version: {cuda_build_version}\")\n",
    "    \n",
    "    # Check 2: Is CUDA currently available for use?\n",
    "    cuda_is_available = torch.cuda.is_available()\n",
    "    print(f\"torch.cuda.is_available(): {cuda_is_available}\")\n",
    "\n",
    "    if not cuda_is_available:\n",
    "        print(\"\\n\\t*** DIAGNOSIS: CUDA is NOT available to PyTorch. ***\")\n",
    "        print(\"\\tCheck your PyTorch installation (did you use the CUDA version?)\")\n",
    "        print(\"\\tCheck your NVIDIA drivers and CUDA Toolkit installation.\")\n",
    "        return\n",
    "\n",
    "    # If CUDA is available, check GPU details\n",
    "    print(\"-\" * 40)\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs detected: {gpu_count}\")\n",
    "    \n",
    "    if gpu_count > 0:\n",
    "        for i in range(gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            print(f\"GPU {i} Name: {gpu_name}\")\n",
    "            print(f\"GPU {i} Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "            print(f\"Total Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Check 3: CuDNN status (often used for performance)\n",
    "    cudnn_is_available = torch.backends.cudnn.is_available()\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"torch.backends.cudnn.is_available(): {cudnn_is_available}\")\n",
    "\n",
    "    if cuda_is_available and gpu_count > 0:\n",
    "        # Final check: can we move a tensor to the device?\n",
    "        try:\n",
    "            test_tensor = torch.randn(2, 2).to('cuda:0')\n",
    "            print(f\"Test tensor successfully moved to device: {test_tensor.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move test tensor to GPU: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_cuda_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and dummy input moved to cuda:0 successfully.\n",
      "Warming up for 10 iterations on CUDA...\n",
      "Warm-up complete. Starting benchmark...\n",
      "\n",
      "--- Benchmark Results (CUDA @ BATCH=64) ---\n",
      "Inference Time (Avg over 100 runs): 41.440 ms\n",
      "Throughput (FPS): 1544.40 FPS\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- TABLE 1: DETAILED FP32 BASELINE METRICS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "      <th>Unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Target Hardware</td>\n",
       "      <td>NVIDIA GeForce RTX 3060 Laptop GPU</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inference Device</td>\n",
       "      <td>CUDA</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batch Size</td>\n",
       "      <td>64</td>\n",
       "      <td>Samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Size (FP32)</td>\n",
       "      <td>13.37</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Parameters</td>\n",
       "      <td>3,504,872</td>\n",
       "      <td>Params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Load Time</td>\n",
       "      <td>103.38</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avg. Latency (P50)</td>\n",
       "      <td>41.581</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Avg. Latency (Mean)</td>\n",
       "      <td>41.440</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Worst-Case Latency (P99)</td>\n",
       "      <td>42.467</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Throughput (FPS)</td>\n",
       "      <td>1544.40</td>\n",
       "      <td>FPS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Metric                               Value      Unit\n",
       "0           Target Hardware  NVIDIA GeForce RTX 3060 Laptop GPU       N/A\n",
       "1          Inference Device                                CUDA       N/A\n",
       "2                Batch Size                                  64   Samples\n",
       "3         Model Size (FP32)                               13.37        MB\n",
       "4          Total Parameters                           3,504,872    Params\n",
       "5           Model Load Time                              103.38        ms\n",
       "6        Avg. Latency (P50)                              41.581  ms/batch\n",
       "7       Avg. Latency (Mean)                              41.440  ms/batch\n",
       "8  Worst-Case Latency (P99)                              42.467  ms/batch\n",
       "9          Throughput (FPS)                             1544.40       FPS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- TABLE 2: PROJECT PLAN AND NEXT STEPS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Framework</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Measured Latency (ms)</th>\n",
       "      <th>Measured Throughput (FPS)</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PyTorch (Native)</td>\n",
       "      <td>FP32 (Baseline)</td>\n",
       "      <td>41.44</td>\n",
       "      <td>1544.40</td>\n",
       "      <td>Complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TensorRT</td>\n",
       "      <td>FP16</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TensorRT</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFLite</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OpenVINO</td>\n",
       "      <td>FP32/FP16</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ExecuTorch</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Framework        Precision Measured Latency (ms)  \\\n",
       "0  PyTorch (Native)  FP32 (Baseline)                 41.44   \n",
       "1          TensorRT             FP16         N/A (Planned)   \n",
       "2          TensorRT             INT8         N/A (Planned)   \n",
       "3            TFLite             INT8         N/A (Planned)   \n",
       "4          OpenVINO        FP32/FP16         N/A (Planned)   \n",
       "5        ExecuTorch             INT8         N/A (Planned)   \n",
       "\n",
       "  Measured Throughput (FPS)       Status  \n",
       "0                   1544.40     Complete  \n",
       "1             N/A (Planned)  In Progress  \n",
       "2             N/A (Planned)      Planned  \n",
       "3             N/A (Planned)      Planned  \n",
       "4             N/A (Planned)      Planned  \n",
       "5             N/A (Planned)      Planned  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Global Configuration ---\n",
    "BATCH_SIZE = 64 \n",
    "NUM_WARMUP = 10\n",
    "NUM_BENCHMARK = 100\n",
    "# Define the target device. We will assume CUDA is preferred if available.\n",
    "TARGET_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"MobileNetV2\"\n",
    "if TARGET_DEVICE.type == 'cuda':\n",
    "    # This fetches the specific GPU name (e.g., \"NVIDIA Jetson Nano\", \"RTX 3090\")\n",
    "    DEVICE_NAME = torch.cuda.get_device_name(0)\n",
    "else:\n",
    "    # If no CUDA device, it defaults to CPU\n",
    "    DEVICE_NAME = \"CPU\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# METRIC 1 & 2: MODEL LOAD TIME & MODEL SIZE CALCULATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def calculate_static_metrics():\n",
    "    \"\"\"Calculates model load time and size metrics.\"\"\"\n",
    "    \n",
    "    # Start timer for model loading\n",
    "    start_load_time = time.perf_counter()\n",
    "\n",
    "    # Load MobileNetV2 model\n",
    "    torch_model = torchvision.models.mobilenet_v2(\n",
    "        weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    torch_model.eval()\n",
    "\n",
    "    # Move model to the TARGET_DEVICE (e.g., CUDA)\n",
    "    torch_model.to(TARGET_DEVICE)\n",
    "\n",
    "    # Calculate load time\n",
    "    model_load_time_ms = (time.perf_counter() - start_load_time) * 1000\n",
    "\n",
    "    # Calculate model size (Static Metric)\n",
    "    total_params = sum(p.numel() for p in torch_model.parameters())\n",
    "    # Assuming FP32: 4 bytes per parameter. Convert to Megabytes.\n",
    "    model_size_mb = (total_params * 4) / (1024 * 1024)\n",
    "    \n",
    "    return torch_model, model_load_time_ms, model_size_mb, total_params\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# BENCHMARKING FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def benchmark_model(model, input_tensor, num_warmup, num_benchmark):\n",
    "    \"\"\"\n",
    "    Benchmarks the model inference time, calculating full latency statistics.\n",
    "    \"\"\"\n",
    "    timings = [] # List to store all individual run times\n",
    "    device_type = input_tensor.device.type\n",
    "    \n",
    "    # --- CUDA TIMING LOGIC ---\n",
    "    if device_type == 'cuda':\n",
    "        starter = torch.cuda.Event(enable_timing=True)\n",
    "        ender = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        print(f\"Warming up for {num_warmup} iterations on CUDA...\")\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup):\n",
    "                _ = model(input_tensor)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Warm-up complete. Starting benchmark...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_benchmark):\n",
    "                starter.record()\n",
    "                _ = model(input_tensor)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize() # Wait for GPU\n",
    "                \n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time) # Time is in milliseconds (ms)\n",
    "\n",
    "    # --- CPU TIMING LOGIC ---\n",
    "    else: \n",
    "        print(f\"Warming up for {num_warmup} iterations on CPU...\")\n",
    "        \n",
    "        # Ensure the model is on CPU if the device is CPU\n",
    "        model_on_cpu = model.to('cpu') \n",
    "        input_on_cpu = input_tensor.to('cpu')\n",
    "        \n",
    "        # Warm-up\n",
    "        for _ in range(num_warmup):\n",
    "            with torch.no_grad():\n",
    "                _ = model_on_cpu(input_on_cpu) \n",
    "            \n",
    "        print(\"Warm-up complete. Starting benchmark...\")\n",
    "\n",
    "        # Measure performance\n",
    "        for _ in range(num_benchmark):\n",
    "            start_time = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = model_on_cpu(input_on_cpu)\n",
    "            end_time = time.perf_counter()\n",
    "            timings.append((end_time - start_time) * 1000) # Time in milliseconds (ms)\n",
    "            \n",
    "        # Move the model back to the TARGET_DEVICE\n",
    "        model.to(TARGET_DEVICE)\n",
    "\n",
    "    # --- STATISTICAL CALCULATIONS ---\n",
    "    # Convert timings to a NumPy array for robust percentile calculation\n",
    "    timings_np = np.array(timings)\n",
    "    \n",
    "    mean_time_ms = timings_np.mean()\n",
    "    std_time_ms = timings_np.std()\n",
    "    \n",
    "    median_latency = np.percentile(timings_np, 50)\n",
    "    p90_latency = np.percentile(timings_np, 90)\n",
    "    p99_latency = np.percentile(timings_np, 99)\n",
    "    \n",
    "    # METRIC 4: THROUGHPUT (FPS)\n",
    "    throughput_fps = (BATCH_SIZE / mean_time_ms) * 1000\n",
    "\n",
    "    print(f\"\\n--- Benchmark Results ({device_type.upper()} @ BATCH={input_tensor.shape[0]}) ---\")\n",
    "    print(f\"Inference Time (Avg over {num_benchmark} runs): {mean_time_ms:.3f} ms\")\n",
    "    print(f\"Throughput (FPS): {throughput_fps:.2f} FPS\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    return {\n",
    "        'mean_time_ms': mean_time_ms,\n",
    "        'std_time_ms': std_time_ms,\n",
    "        'median_latency': median_latency,\n",
    "        'p90_latency': p90_latency,\n",
    "        'p99_latency': p99_latency,\n",
    "        'throughput_fps': throughput_fps,\n",
    "        'device_type': device_type,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# PANDAS TABLE GENERATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def generate_presentation_tables(static_metrics, dynamic_metrics):\n",
    "    \"\"\"Generates and prints the two presentation tables.\"\"\"\n",
    "    \n",
    "    # --- TABLE 1: DETAILED BASELINE METRICS (FINDINGS) ---\n",
    "    \n",
    "    metrics_data = {\n",
    "        'Metric': [\n",
    "            'Target Hardware', 'Inference Device', 'Batch Size', 'Model Size (FP32)', \n",
    "            'Total Parameters', 'Model Load Time', 'Avg. Latency (P50)', \n",
    "            'Avg. Latency (Mean)', 'Worst-Case Latency (P99)', 'Throughput (FPS)'\n",
    "        ],\n",
    "        'Value': [\n",
    "            DEVICE_NAME, \n",
    "            dynamic_metrics['device_type'].upper(), \n",
    "            dynamic_metrics['batch_size'], \n",
    "            f\"{static_metrics['model_size_mb']:.2f}\",\n",
    "            f\"{static_metrics['total_params']:,}\",\n",
    "            f\"{static_metrics['model_load_time_ms']:.2f}\",\n",
    "            f\"{dynamic_metrics['median_latency']:.3f}\",\n",
    "            f\"{dynamic_metrics['mean_time_ms']:.3f}\",\n",
    "            f\"{dynamic_metrics['p99_latency']:.3f}\",\n",
    "            f\"{dynamic_metrics['throughput_fps']:.2f}\"\n",
    "        ],\n",
    "        'Unit': [\n",
    "            'N/A', 'N/A', 'Samples', 'MB', 'Params', 'ms', 'ms/batch', 'ms/batch', 'ms/batch', 'FPS'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_data)\n",
    "    print(\"\\n\\n--- TABLE 1: DETAILED FP32 BASELINE METRICS ---\")\n",
    "    display(df_metrics)\n",
    "    \n",
    "    # --- TABLE 2: PROJECT PLAN (NEXT STEPS) ---\n",
    "\n",
    "\n",
    "    next_steps_data = {\n",
    "        'Framework': [\n",
    "            'PyTorch (Native)', \n",
    "            'TensorRT', \n",
    "            'TensorRT', \n",
    "            'TFLite', \n",
    "            'OpenVINO', \n",
    "            'ExecuTorch'\n",
    "        ],\n",
    "        'Precision': [\n",
    "            f'FP32 (Baseline)', \n",
    "            'FP16', \n",
    "            'INT8', \n",
    "            'INT8', \n",
    "            'FP32/FP16', \n",
    "            'INT8'\n",
    "        ],\n",
    "        'Measured Latency (ms)': [\n",
    "            f'{dynamic_metrics[\"mean_time_ms\"]:.2f}', # Measured Value\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)'\n",
    "        ],\n",
    "        'Measured Throughput (FPS)': [\n",
    "            f'{dynamic_metrics[\"throughput_fps\"]:.2f}', # Measured Value\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)'\n",
    "        ],\n",
    "        'Status': [\n",
    "            'Complete',\n",
    "            'In Progress',\n",
    "            'Planned',\n",
    "            'Planned',\n",
    "            'Planned',\n",
    "            'Planned'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_plan = pd.DataFrame(next_steps_data)\n",
    "    print(\"\\n\\n--- TABLE 2: PROJECT PLAN AND NEXT STEPS ---\")\n",
    "    display(df_plan)\n",
    "\n",
    "    # MAIN\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1. Setup \n",
    "    torch_model, model_load_time_ms, model_size_mb, total_params = calculate_static_metrics()\n",
    "    \n",
    "    static_metrics = {\n",
    "        'model_load_time_ms': model_load_time_ms,\n",
    "        'model_size_mb': model_size_mb,\n",
    "        'total_params': total_params,\n",
    "    }\n",
    "\n",
    "    # Dummy Input\n",
    "    dummy_input = torch.randn(BATCH_SIZE, 3, 224, 224).to(TARGET_DEVICE)\n",
    "    print(f\"Model and dummy input moved to {TARGET_DEVICE} successfully.\")\n",
    "\n",
    "    # 2. Benchmarking\n",
    "    dynamic_metrics = benchmark_model(torch_model, dummy_input, NUM_WARMUP, NUM_BENCHMARK)\n",
    "    \n",
    "    # 3. Generate Tables\n",
    "    generate_presentation_tables(static_metrics, dynamic_metrics)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7edcbd51d003786adb1b4e1f80280b9023b7f40ffcc7c654fe654e28bb9b8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
