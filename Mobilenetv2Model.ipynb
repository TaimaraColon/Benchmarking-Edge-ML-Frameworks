{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and dummy input moved to cpu successfully.\n",
      "Warming up for 10 iterations on CPU...\n",
      "Warm-up complete. Starting benchmark...\n",
      "\n",
      "--- Benchmark Results (CPU @ BATCH=64) ---\n",
      "Inference Time (Avg over 100 runs): 499.195 ms\n",
      "Throughput (FPS): 128.21 FPS\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- TABLE 1: DETAILED FP32 BASELINE METRICS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "      <th>Unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Target Hardware</td>\n",
       "      <td>CPU</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inference Device</td>\n",
       "      <td>CPU</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batch Size</td>\n",
       "      <td>64</td>\n",
       "      <td>Samples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model Size (FP32)</td>\n",
       "      <td>13.37</td>\n",
       "      <td>MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Parameters</td>\n",
       "      <td>3,504,872</td>\n",
       "      <td>Params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model Load Time</td>\n",
       "      <td>57.72</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avg. Latency (P50)</td>\n",
       "      <td>498.919</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Avg. Latency (Mean)</td>\n",
       "      <td>499.195</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Worst-Case Latency (P99)</td>\n",
       "      <td>518.864</td>\n",
       "      <td>ms/batch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Throughput (FPS)</td>\n",
       "      <td>128.21</td>\n",
       "      <td>FPS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Metric      Value      Unit\n",
       "0           Target Hardware        CPU       N/A\n",
       "1          Inference Device        CPU       N/A\n",
       "2                Batch Size         64   Samples\n",
       "3         Model Size (FP32)      13.37        MB\n",
       "4          Total Parameters  3,504,872    Params\n",
       "5           Model Load Time      57.72        ms\n",
       "6        Avg. Latency (P50)    498.919  ms/batch\n",
       "7       Avg. Latency (Mean)    499.195  ms/batch\n",
       "8  Worst-Case Latency (P99)    518.864  ms/batch\n",
       "9          Throughput (FPS)     128.21       FPS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- TABLE 2: PROJECT PLAN AND NEXT STEPS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Framework</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Measured Latency (ms)</th>\n",
       "      <th>Measured Throughput (FPS)</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PyTorch (Native)</td>\n",
       "      <td>FP32 (Baseline)</td>\n",
       "      <td>499.20</td>\n",
       "      <td>128.21</td>\n",
       "      <td>Complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TensorRT</td>\n",
       "      <td>FP16</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TensorRT</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFLite</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OpenVINO</td>\n",
       "      <td>FP32/FP16</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ExecuTorch</td>\n",
       "      <td>INT8</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>N/A (Planned)</td>\n",
       "      <td>Planned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Framework        Precision Measured Latency (ms)  \\\n",
       "0  PyTorch (Native)  FP32 (Baseline)                499.20   \n",
       "1          TensorRT             FP16         N/A (Planned)   \n",
       "2          TensorRT             INT8         N/A (Planned)   \n",
       "3            TFLite             INT8         N/A (Planned)   \n",
       "4          OpenVINO        FP32/FP16         N/A (Planned)   \n",
       "5        ExecuTorch             INT8         N/A (Planned)   \n",
       "\n",
       "  Measured Throughput (FPS)       Status  \n",
       "0                    128.21     Complete  \n",
       "1             N/A (Planned)  In Progress  \n",
       "2             N/A (Planned)      Planned  \n",
       "3             N/A (Planned)      Planned  \n",
       "4             N/A (Planned)      Planned  \n",
       "5             N/A (Planned)      Planned  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Global Configuration ---\n",
    "BATCH_SIZE = 64 \n",
    "NUM_WARMUP = 10\n",
    "NUM_BENCHMARK = 100\n",
    "# Define the target device. We will assume CUDA is preferred if available.\n",
    "TARGET_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"MobileNetV2\"\n",
    "if TARGET_DEVICE.type == 'cuda':\n",
    "    # This fetches the specific GPU name (e.g., \"NVIDIA Jetson Nano\", \"RTX 3090\")\n",
    "    DEVICE_NAME = torch.cuda.get_device_name(0)\n",
    "else:\n",
    "    # If no CUDA device, it defaults to CPU\n",
    "    DEVICE_NAME = \"CPU\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# METRIC 1 & 2: MODEL LOAD TIME & MODEL SIZE CALCULATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def calculate_static_metrics():\n",
    "    \"\"\"Calculates model load time and size metrics.\"\"\"\n",
    "    \n",
    "    # Start timer for model loading\n",
    "    start_load_time = time.perf_counter()\n",
    "\n",
    "    # Load MobileNetV2 model\n",
    "    torch_model = torchvision.models.mobilenet_v2(\n",
    "        weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    torch_model.eval()\n",
    "\n",
    "    # Move model to the TARGET_DEVICE (e.g., CUDA)\n",
    "    torch_model.to(TARGET_DEVICE)\n",
    "\n",
    "    # Calculate load time\n",
    "    model_load_time_ms = (time.perf_counter() - start_load_time) * 1000\n",
    "\n",
    "    # Calculate model size (Static Metric)\n",
    "    total_params = sum(p.numel() for p in torch_model.parameters())\n",
    "    # Assuming FP32: 4 bytes per parameter. Convert to Megabytes.\n",
    "    model_size_mb = (total_params * 4) / (1024 * 1024)\n",
    "    \n",
    "    return torch_model, model_load_time_ms, model_size_mb, total_params\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# BENCHMARKING FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def benchmark_model(model, input_tensor, num_warmup, num_benchmark):\n",
    "    \"\"\"\n",
    "    Benchmarks the model inference time, calculating full latency statistics.\n",
    "    \"\"\"\n",
    "    timings = [] # List to store all individual run times\n",
    "    device_type = input_tensor.device.type\n",
    "    \n",
    "    # --- CUDA TIMING LOGIC ---\n",
    "    if device_type == 'cuda':\n",
    "        starter = torch.cuda.Event(enable_timing=True)\n",
    "        ender = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        print(f\"Warming up for {num_warmup} iterations on CUDA...\")\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup):\n",
    "                _ = model(input_tensor)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Warm-up complete. Starting benchmark...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_benchmark):\n",
    "                starter.record()\n",
    "                _ = model(input_tensor)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize() # Wait for GPU\n",
    "                \n",
    "                curr_time = starter.elapsed_time(ender)\n",
    "                timings.append(curr_time) # Time is in milliseconds (ms)\n",
    "\n",
    "    # --- CPU TIMING LOGIC ---\n",
    "    else: \n",
    "        print(f\"Warming up for {num_warmup} iterations on CPU...\")\n",
    "        \n",
    "        # Ensure the model is on CPU if the device is CPU\n",
    "        model_on_cpu = model.to('cpu') \n",
    "        input_on_cpu = input_tensor.to('cpu')\n",
    "        \n",
    "        # Warm-up\n",
    "        for _ in range(num_warmup):\n",
    "            with torch.no_grad():\n",
    "                _ = model_on_cpu(input_on_cpu) \n",
    "            \n",
    "        print(\"Warm-up complete. Starting benchmark...\")\n",
    "\n",
    "        # Measure performance\n",
    "        for _ in range(num_benchmark):\n",
    "            start_time = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = model_on_cpu(input_on_cpu)\n",
    "            end_time = time.perf_counter()\n",
    "            timings.append((end_time - start_time) * 1000) # Time in milliseconds (ms)\n",
    "            \n",
    "        # Move the model back to the TARGET_DEVICE\n",
    "        model.to(TARGET_DEVICE)\n",
    "\n",
    "    # --- STATISTICAL CALCULATIONS ---\n",
    "    # Convert timings to a NumPy array for robust percentile calculation\n",
    "    timings_np = np.array(timings)\n",
    "    \n",
    "    mean_time_ms = timings_np.mean()\n",
    "    std_time_ms = timings_np.std()\n",
    "    \n",
    "    median_latency = np.percentile(timings_np, 50)\n",
    "    p90_latency = np.percentile(timings_np, 90)\n",
    "    p99_latency = np.percentile(timings_np, 99)\n",
    "    \n",
    "    # METRIC 4: THROUGHPUT (FPS)\n",
    "    throughput_fps = (BATCH_SIZE / mean_time_ms) * 1000\n",
    "\n",
    "    print(f\"\\n--- Benchmark Results ({device_type.upper()} @ BATCH={input_tensor.shape[0]}) ---\")\n",
    "    print(f\"Inference Time (Avg over {num_benchmark} runs): {mean_time_ms:.3f} ms\")\n",
    "    print(f\"Throughput (FPS): {throughput_fps:.2f} FPS\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    return {\n",
    "        'mean_time_ms': mean_time_ms,\n",
    "        'std_time_ms': std_time_ms,\n",
    "        'median_latency': median_latency,\n",
    "        'p90_latency': p90_latency,\n",
    "        'p99_latency': p99_latency,\n",
    "        'throughput_fps': throughput_fps,\n",
    "        'device_type': device_type,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# PANDAS TABLE GENERATION\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def generate_presentation_tables(static_metrics, dynamic_metrics):\n",
    "    \"\"\"Generates and prints the two presentation tables.\"\"\"\n",
    "    \n",
    "    # --- TABLE 1: DETAILED BASELINE METRICS (FINDINGS) ---\n",
    "    \n",
    "    metrics_data = {\n",
    "        'Metric': [\n",
    "            'Target Hardware', 'Inference Device', 'Batch Size', 'Model Size (FP32)', \n",
    "            'Total Parameters', 'Model Load Time', 'Avg. Latency (P50)', \n",
    "            'Avg. Latency (Mean)', 'Worst-Case Latency (P99)', 'Throughput (FPS)'\n",
    "        ],\n",
    "        'Value': [\n",
    "            DEVICE_NAME, \n",
    "            dynamic_metrics['device_type'].upper(), \n",
    "            dynamic_metrics['batch_size'], \n",
    "            f\"{static_metrics['model_size_mb']:.2f}\",\n",
    "            f\"{static_metrics['total_params']:,}\",\n",
    "            f\"{static_metrics['model_load_time_ms']:.2f}\",\n",
    "            f\"{dynamic_metrics['median_latency']:.3f}\",\n",
    "            f\"{dynamic_metrics['mean_time_ms']:.3f}\",\n",
    "            f\"{dynamic_metrics['p99_latency']:.3f}\",\n",
    "            f\"{dynamic_metrics['throughput_fps']:.2f}\"\n",
    "        ],\n",
    "        'Unit': [\n",
    "            'N/A', 'N/A', 'Samples', 'MB', 'Params', 'ms', 'ms/batch', 'ms/batch', 'ms/batch', 'FPS'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_data)\n",
    "    print(\"\\n\\n--- TABLE 1: DETAILED FP32 BASELINE METRICS ---\")\n",
    "    display(df_metrics)\n",
    "    \n",
    "    # --- TABLE 2: PROJECT PLAN (NEXT STEPS) ---\n",
    "\n",
    "\n",
    "    next_steps_data = {\n",
    "        'Framework': [\n",
    "            'PyTorch (Native)', \n",
    "            'TensorRT', \n",
    "            'TensorRT', \n",
    "            'TFLite', \n",
    "            'OpenVINO', \n",
    "            'ExecuTorch'\n",
    "        ],\n",
    "        'Precision': [\n",
    "            f'FP32 (Baseline)', \n",
    "            'FP16', \n",
    "            'INT8', \n",
    "            'INT8', \n",
    "            'FP32/FP16', \n",
    "            'INT8'\n",
    "        ],\n",
    "        'Measured Latency (ms)': [\n",
    "            f'{dynamic_metrics[\"mean_time_ms\"]:.2f}', # Measured Value\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)'\n",
    "        ],\n",
    "        'Measured Throughput (FPS)': [\n",
    "            f'{dynamic_metrics[\"throughput_fps\"]:.2f}', # Measured Value\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)',\n",
    "            'N/A (Planned)'\n",
    "        ],\n",
    "        'Status': [\n",
    "            'Complete',\n",
    "            'In Progress',\n",
    "            'Planned',\n",
    "            'Planned',\n",
    "            'Planned',\n",
    "            'Planned'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_plan = pd.DataFrame(next_steps_data)\n",
    "    print(\"\\n\\n--- TABLE 2: PROJECT PLAN AND NEXT STEPS ---\")\n",
    "    display(df_plan)\n",
    "\n",
    "    # MAIN\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1. Setup \n",
    "    torch_model, model_load_time_ms, model_size_mb, total_params = calculate_static_metrics()\n",
    "    \n",
    "    static_metrics = {\n",
    "        'model_load_time_ms': model_load_time_ms,\n",
    "        'model_size_mb': model_size_mb,\n",
    "        'total_params': total_params,\n",
    "    }\n",
    "\n",
    "    # Dummy Input\n",
    "    dummy_input = torch.randn(BATCH_SIZE, 3, 224, 224).to(TARGET_DEVICE)\n",
    "    print(f\"Model and dummy input moved to {TARGET_DEVICE} successfully.\")\n",
    "\n",
    "    # 2. Benchmarking\n",
    "    dynamic_metrics = benchmark_model(torch_model, dummy_input, NUM_WARMUP, NUM_BENCHMARK)\n",
    "    \n",
    "    # 3. Generate Tables\n",
    "    generate_presentation_tables(static_metrics, dynamic_metrics)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7edcbd51d003786adb1b4e1f80280b9023b7f40ffcc7c654fe654e28bb9b8e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
